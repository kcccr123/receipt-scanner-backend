{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import callbacks\n",
    "import data\n",
    "import dataUtils as du\n",
    "import metric\n",
    "import modelArc\n",
    "import trainer\n",
    "from config import ConfigFile\n",
    "from datetime import datetime\n",
    "import json as js\n",
    "import numpy as np\n",
    "import cv2\n",
    "import typing\n",
    "from skimage.filters import threshold_local\n",
    "\n",
    "def bw_scanner(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    T = threshold_local(gray, 21, offset = 5, method = \"gaussian\")\n",
    "    return (gray > T).astype(\"uint8\") * 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    # Load the image using OpenCV\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    blur = cv2.GaussianBlur(gray, (3,3), 0)\n",
    "    thresh = cv2.adaptiveThreshold(blur, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 15, 4)\n",
    "    # thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "    \n",
    "    # Morph open to remove noise and invert image\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))\n",
    "    opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "    # invert = 255 - opening\n",
    "\n",
    "    return opening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset\n",
      "dataset1 done\n",
      "loading dataset\n",
      "dataset1 done\n",
      "loading dataset\n",
      "dataset2 done\n",
      "42902\n",
      "database, vocab, max_len, complete\n"
     ]
    }
   ],
   "source": [
    "#Specify path to main database\n",
    "data_dir = r\"D:\\photos\\RCNN4\\BBOXES\"\n",
    "model_path = r\"D:\\Projects\\reciept-scanner\\RCNN\\models\"\n",
    "database, vocab, max_len = [], set(), 0\n",
    "\n",
    "#load second dataset\n",
    "data_path = r\"D:\\photos\\SORIE\"\n",
    "\n",
    "path = os.path.join(data_path, \"train\").replace(\"\\\\\",\"/\")\n",
    "i = 1\n",
    "while i <= 2:\n",
    "    print(\"loading dataset\")\n",
    "    with open(os.path.join(path, \"metadata.jsonl\").replace(\"\\\\\",\"/\"), 'r') as file:\n",
    "        for line in file:\n",
    "            \n",
    "            row = js.loads(line)\n",
    "            img_path = os.path.join(path, row.get(\"file_name\")).replace(\"\\\\\",\"/\")\n",
    "            if os.path.exists(img_path):\n",
    "                label = row.get(\"text\").rstrip(\"\\n\")\n",
    "                if len(label.split(' ')) <= 3 and label != \"***\": \n",
    "                    vocab.update(list(label))\n",
    "                    max_len = max(max_len, len(label))\n",
    "                    database.append([img_path, label])\n",
    "            else:\n",
    "                print(\"image with path \" + str(img_path) + \" do not exist\")\n",
    "    if i == 1:\n",
    "        print(\"dataset1 done\")\n",
    "    \n",
    "    i += 1\n",
    "    path = os.path.join(data_path, \"test\").replace(\"\\\\\",\"/\")\n",
    "\n",
    "print(\"dataset1 done\")\n",
    "\n",
    "print(\"loading dataset\")\n",
    "with open(os.path.join(path, \"testing.jsonl\").replace(\"\\\\\",\"/\"), 'r') as file:\n",
    "    for line in file:\n",
    "        \n",
    "        row = js.loads(line)\n",
    "        img_path = os.path.join(path, row.get(\"file_name\")).replace(\"\\\\\",\"/\")\n",
    "        if os.path.exists(img_path):\n",
    "            label = row.get(\"text\").rstrip(\"\\n\")\n",
    "            if len(label.split(' ')) <= 3 and label != \"***\": \n",
    "                vocab.update(list(label))\n",
    "                max_len = max(max_len, len(label))\n",
    "                database.append([img_path, label])\n",
    "        else:\n",
    "            print(\"image with path \" + str(img_path) + \" do not exist\")\n",
    "print(\"dataset2 done\")\n",
    "print(len(database))\n",
    "\n",
    "print(\"database, vocab, max_len, complete\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "height: 35.46871940702065\n",
      "width: 147.15467810358493\n"
     ]
    }
   ],
   "source": [
    "#normalize images and load them as an np array\n",
    "\n",
    "#get the avg height and width of dataset\n",
    "num_images = 0.0\n",
    "height_all = 0.0\n",
    "width_all = 0.0\n",
    "\n",
    "for pair in database:\n",
    "    image_path = pair[0]\n",
    "    image = cv2.imread(image_path)\n",
    "    img = preprocess_image(image)\n",
    "\n",
    "    if img is None:\n",
    "        print(f\"Warning: Could not read image at {image_path}. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    height, width = img.shape\n",
    "\n",
    "    num_images += 1\n",
    "    height_all += height\n",
    "    width_all += width\n",
    "\n",
    "h = height_all/num_images\n",
    "w = width_all/num_images\n",
    "print(\"height:\", h)\n",
    "print(\"width:\", w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: [193.16035]\n",
      "Standard Deviation: [109.291824]\n"
     ]
    }
   ],
   "source": [
    "#normalize images and load them as an np array\n",
    "\n",
    "#get the mean and std of dataset\n",
    "num_pixels = 0\n",
    "channel_sum = torch.tensor([0.0])\n",
    "channel_sum_squared = torch.tensor([0.0])\n",
    "\n",
    "for pair in database:\n",
    "    image_path = pair[0]\n",
    "    image = cv2.imread(image_path)\n",
    "    img = preprocess_image(image)\n",
    "\n",
    "    if img is None:\n",
    "        print(f\"Warning: Could not read image at {image_path}. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    height, width = img.shape\n",
    "\n",
    "    num_pixels += height * width\n",
    "\n",
    "    channel_sum += torch.tensor(img.sum(), dtype=torch.float64)\n",
    "    channel_sum_squared += torch.tensor((img.astype(np.float64) ** 2).sum(), dtype=torch.float64)\n",
    "\n",
    "mean = channel_sum/num_pixels\n",
    "variance = (channel_sum_squared / num_pixels) - (mean**2)\n",
    "std = torch.sqrt(variance)\n",
    "print(\"Mean:\", mean.numpy())\n",
    "print(\"Standard Deviation:\", std.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import image\n",
    "class CVImage(image.Image):\n",
    "    #Image class for storing image data and metadata (opencv based)\n",
    "\n",
    "    init_successful = False\n",
    "\n",
    "    def __init__(self, image: typing.Union[str, np.ndarray], method: int = cv2.IMREAD_COLOR,\n",
    "                path: str = \"\", color: str = \"RGB\", mean: torch.tensor = mean, std: torch.tensor = std) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        if isinstance(image, str):#image path\n",
    "            if not os.path.exists(image):\n",
    "                raise FileNotFoundError(f\"Image {image} not found.\")\n",
    "\n",
    "            self._image = cv2.imread(image)\n",
    "            self.path = image\n",
    "            self.color = \"GREY\"\n",
    "            self._image = bw_scanner(self._image)\n",
    "\n",
    "        elif isinstance(image, np.ndarray):\n",
    "            self._image = image\n",
    "            self.path = path\n",
    "            self.color = color\n",
    "\n",
    "        else:\n",
    "            raise TypeError(f\"Image must be either path to image or numpy.ndarray, not {type(image)}\")\n",
    "\n",
    "        self.method = method\n",
    "\n",
    "        if self._image is None:\n",
    "            return None\n",
    "        \n",
    "        #start normalizing image\n",
    "        if mean != None and std != None:\n",
    "            img_tensor = torch.tensor(self._image, dtype=torch.float32)\n",
    "            norm_img = (img_tensor - mean) / std\n",
    "            self._image = norm_img.numpy()\n",
    "\n",
    "        self.init_successful = True\n",
    "\n",
    "        # save width, height and channels\n",
    "        self.width = self._image.shape[1]\n",
    "        self.height = self._image.shape[0]\n",
    "        self.channels = 1 if len(self._image.shape) == 2 else self._image.shape[2]\n",
    "\n",
    "    @property\n",
    "    def image(self) -> np.ndarray:\n",
    "        return self._image\n",
    "\n",
    "    @image.setter\n",
    "    def image(self, value: np.ndarray):\n",
    "        self._image = value\n",
    "\n",
    "    @property\n",
    "    def shape(self) -> tuple:\n",
    "        return self._image.shape\n",
    "\n",
    "    @property\n",
    "    def center(self) -> tuple:\n",
    "        return self.width // 2, self.height // 2\n",
    "\n",
    "    def update(self, image: np.ndarray):\n",
    "        if isinstance(image, np.ndarray):\n",
    "            self._image = image\n",
    "\n",
    "            # save width, height and channels\n",
    "            self.width = self._image.shape[1]\n",
    "            self.height = self._image.shape[0]\n",
    "            self.channels = 1 if len(self._image.shape) == 2 else self._image.shape[2]\n",
    "\n",
    "            return self\n",
    "\n",
    "        else:\n",
    "            raise TypeError(f\"image must be numpy.ndarray, not {type(image)}\")\n",
    "        \n",
    "    def RGB(self) -> np.ndarray:\n",
    "        if self.color == \"RGB\":\n",
    "            return self._image\n",
    "        elif self.color == \"BGR\":\n",
    "            return cv2.cvtColor(self._image, cv2.COLOR_BGR2RGB)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown color format {self.color}\")\n",
    "        \n",
    "    def HSV(self) -> np.ndarray:\n",
    "        if self.color == \"BGR\":\n",
    "            return cv2.cvtColor(self._image, cv2.COLOR_BGR2HSV)\n",
    "        elif self.color == \"RGB\":\n",
    "            return cv2.cvtColor(self._image, cv2.COLOR_RGB2HSV)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown color format {self.color}\")\n",
    "\n",
    "    def flip(self, axis: int = 0):\n",
    "        #flip image along x or y axis\n",
    "        if axis not in [0, 1]:\n",
    "            raise ValueError(f\"axis must be either 0 or 1, not {axis}\")\n",
    "\n",
    "        self._image = self._image[:, ::-1] if axis == 0 else self._image[::-1]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def numpy(self) -> np.ndarray:\n",
    "        return self._image\n",
    "    \n",
    "    def __call__(self) -> np.ndarray:\n",
    "        return self._image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config saved toD:/Projects/reciept-scanner/RCNN/models/202408282345\n"
     ]
    }
   ],
   "source": [
    "#create data loaders\n",
    "model_config = ConfigFile(name = \"CRNN1\", path = model_path, lr=0.0004, bs=32)\n",
    "\n",
    "model_config.vocab = \"\".join(vocab)\n",
    "model_config.max_txt_len = max_len\n",
    "model_config.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done coopying\n"
     ]
    }
   ],
   "source": [
    "dataset_loader = data.DataLoader(dataset = database, batch_size = model_config.batch_size, \n",
    "                                 data_preprocessors = [image.ImageReader(CVImage)], \n",
    "                                 transformers = [du.ImageResizer(model_config.width, model_config.height), du.LabelIndexer(model_config.vocab), \n",
    "                                                 du.LabelPadding(padding_value = len(model_config.vocab), max_word_len = max_len)])# du.ImageShowCV2()\n",
    "\n",
    "\n",
    "train_set, val_set = dataset_loader.split(split = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Enabled...Training On GPU\n"
     ]
    }
   ],
   "source": [
    "train_set.augmentors = [\n",
    "    # du.RandomBrightness(),\n",
    "    du.RandomErodeDilate(),\n",
    "    du.RandomSharpen(),\n",
    "    du.RandomRotate(angle=10),\n",
    "    ]\n",
    "\n",
    "#initialize model, optimizer, and loss\n",
    "model = modelArc.CRNNGREY(len(model_config.vocab))\n",
    "loss = trainer.CTCLoss(blank = len(model_config.vocab))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=model_config.lr)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    print(\"CUDA Enabled...Training On GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialze callbacks and trainer\n",
    "earlystop = callbacks.EarlyStopping(monitor = \"val_CER\", patience = 40, verbose = True)\n",
    "ckpt = callbacks.ModelCheckpoint((model_config.model_path + \"/model.pt\").replace(\"\\\\\",\"/\"), monitor = \"val_CER\", verbose = True)\n",
    "tracker = callbacks.TensorBoard((model_config.model_path + \"/logs\").replace(\"\\\\\",\"/\"))\n",
    "auto_lr = callbacks.ReduceLROnPlateau(monitor = \"val_CER\", factor=0.9, patience = 10, verbose = True)\n",
    "save_model = callbacks.Model2onnx(saved_model_path = (os.path.join(model_path, datetime.strftime(datetime.now(), \"%Y%m%d%H%M\"),\"model.pt\").replace(\"\\\\\",\"/\")), input_shape = (1, model_config.height, model_config.width, 3), verbose = True, metadata = {\"vocab\": model_config.vocab})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_struct = trainer.Trainer(model, optimizer, loss, metrics = [metric.CERMetric(model_config.vocab), metric.WERMetric(model_config.vocab)])\n",
    "\n",
    "#train\n",
    "train_struct.run(train_set, val_set, epochs=1000, callbacks = [ckpt, tracker, auto_lr, save_model, earlystop])\n",
    "\n",
    "train_set.to_csv(os.path.join(model_config.model_path, \"train.csv\").replace(\"\\\\\",\"/\"))\n",
    "val_set.to_csv(os.path.join(model_config.model_path, \"val.csv\").replace(\"\\\\\",\"/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been exported to D:/Projects/reciept-scanner/RCNN/models/202408282357/model.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gary Guo\\anaconda3\\envs\\scannerCRNN\\Lib\\site-packages\\torch\\onnx\\symbolic_opset9.py:4661: UserWarning: Exporting a model to ONNX with a batch_size other than 1, with a variable length with LSTM can cause an error when running the ONNX model with a different batch size. Make sure to save the model with a batch size of 1, or define the initial states (h0/c0) as inputs of the model. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch.onnx\n",
    "from pathlib import Path\n",
    "\n",
    "# Define your CRNNGREY model class here (as provided in your previous messages)\n",
    "vocab = \"37;\\\">iT/:P5(sE<!R2\\\\I8`_SrV}NLd^m=OKlD[e6AB,')U4&@]%?\\xB7.Ma{#$C0Z-*1HW|Yh9cJ~QGX\\\n",
    "  \\ F+n\"\n",
    "# Initialize your model\n",
    "model = modelArc.CRNNGREY(max_chars=len(vocab)-3)  # Adjust max_chars as per your requirement\n",
    "\n",
    "# Load the model weights\n",
    "model.load_state_dict(torch.load('D:/Projects/reciept-scanner/RCNN/models/202408282357/model.pt', map_location=torch.device('cpu')))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = (1, 1, 32, 128)  # Example input shape, replace with your actual input size\n",
    "\n",
    "# Create a dummy input tensor\n",
    "dummy_input = torch.randn((1,) + input_shape[1:])\n",
    "\n",
    "# Define the ONNX file path\n",
    "onnx_model_path = 'D:/Projects/reciept-scanner/RCNN/models/202408282357/model.onnx'\n",
    "\n",
    "# Export the model to ONNX format\n",
    "torch.onnx.export(\n",
    "    model,                    # Model to be exported\n",
    "    dummy_input,              # Dummy input for tracing\n",
    "    onnx_model_path,          # Output file path\n",
    "    export_params=True,       # Store the trained parameter weights\n",
    "    opset_version=14,         # ONNX version to export to\n",
    "    do_constant_folding=True, # Whether to perform constant folding for optimization\n",
    "    input_names=['input'],    # Name of the model's input\n",
    "    output_names=['output'],  # Name of the model's output\n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}  # Dynamic axes for variable-length sequences\n",
    ")\n",
    "\n",
    "print(f\"Model has been exported to {onnx_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scannerCRNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
